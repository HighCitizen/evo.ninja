{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a file in jsonl format\n",
    "import pandas as pd    \n",
    "\n",
    "file_path = \"gaia_dataset.jsonl\"\n",
    "jsonObj = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "\n",
    "# Filter out entries where 'file_name' is not empty\n",
    "jsonObj = jsonObj[jsonObj['file_name'] == \"\"]\n",
    "\n",
    "#Display the dataframe\n",
    "jsonObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "Questions = jsonObj.Question\n",
    "Answers = jsonObj['Final answer']\n",
    "Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for the judging agent which will give us the verdict\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def ask_gpt(gaia_question, final_answer, ground_truth):\n",
    "    # Get the verdict\n",
    "    verdict_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a judge and have to provide justification for your verdict of the third-party. Here is the question that was given to the third-party: {gaia_question}, and the answer the third-party gave us was {final_answer}, while the correct response should be {ground_truth}.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Did the answer received match the correct response? Respond only with a number of 1 to 100 where 100 is a perfect score. \"}\n",
    "        ]\n",
    "    )\n",
    "    verdict = verdict_completion.choices[0].message\n",
    "\n",
    "    print(verdict)\n",
    "    # Ask for justification, including the verdict in the history\n",
    "    justification_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a judge and have to provide justification for your verdict of the third-party. Here is the question that was given to the third-party: {gaia_question}, and the answer the third-party gave us was {final_answer}, while the correct response should be {ground_truth}.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Did the answer received match the correct response? Respond only with a number of 1 to 100 where 100 is a perfect score. \"},\n",
    "            {\"role\": \"system\", \"content\": f\"Your verdict was {verdict}.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Please provide your justification for the verdict.\"}\n",
    "        ]\n",
    "    )\n",
    "    justification = justification_completion.choices[0].message\n",
    "\n",
    "    return (verdict, justification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to validate the answers of an agent,\n",
    "# given a question from the Gaia benchmark and an answer\n",
    "def gaia_validator(gaia_question, final_answer):\n",
    "    \"\"\"Takes in a question and a final answer, checks with openAI if the answer is valid, \n",
    "    Returns boolean\"\"\"\n",
    "    for i in range(len(Questions)):\n",
    "        if gaia_question == Questions[i]:\n",
    "            print(\"Answer should be \", Answers[i])\n",
    "            print(\"Your answer: \", final_answer)\n",
    "            if final_answer == Answers[i]:\n",
    "                return 100\n",
    "            else:\n",
    "                verdict, justification = ask_gpt(gaia_question, final_answer, Answers[i])\n",
    "                print(verdict)\n",
    "                print(justification)\n",
    "                # Remove unwanted characters and convert to integer\n",
    "                cleaned_verdict = ''.join(filter(lambda x: x.isdigit(), verdict.content))\n",
    "                verdict_int = int(cleaned_verdict) if cleaned_verdict else 0  # Converts to int, defaults to 0 if empty\n",
    "\n",
    "                return verdict_int, justification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import click\n",
    "\n",
    "def run_script_and_validate(Questions):\n",
    "    for question in Questions[:10]:  # Iterate over the first ten questions\n",
    "        print(f\"Processing question: {question}\")\n",
    "        try:\n",
    "            process = subprocess.run([\"pyenv\", \"global\", \"3.10.0\"]), \n",
    "            process = subprocess.run([\"pip3\", \"uninstall\", \"pyautogen\"]), \n",
    "\n",
    "            process = subprocess.run([\"pip3\", \"install\", \"pyautogen~=0.2.0b5\"]), \n",
    "\n",
    "            # Run the main.py script with the question and a timeout\n",
    "            process = subprocess.run([\"python3\", \"evopynja/main.py\", \"--goal\", question], \n",
    "                                     capture_output=True, text=True, timeout=300)\n",
    "\n",
    "            if process.returncode != 0:\n",
    "                print(f\"Error in script execution: {process.stderr}\")\n",
    "                continue  # Skip to the next question if there's an error\n",
    "\n",
    "            answer = process.stdout\n",
    "\n",
    "            # Clean up the answer and save to answer.txt\n",
    "            cleaned_answer = clean_answer(answer)\n",
    "            with open(\"answer.txt\", \"w\") as file:\n",
    "                file.write(cleaned_answer)\n",
    "\n",
    "            # Pass the question and answer to the gaia_validator function\n",
    "            gaia_validator(question, cleaned_answer)\n",
    "        \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"Script timed out for question: {question}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def clean_answer(answer):\n",
    "    # Implement cleaning logic here to make it easier to judge the answer\n",
    "    return answer.strip()\n",
    "\n",
    "# Run the function with your questions\n",
    "run_script_and_validate(Questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
